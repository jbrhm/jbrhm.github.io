<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>John's Website</title>
  </head>
<meta charset="UTF-8">
  <body>

    <canvas id = "bg"></canvas>

    <main>
      <section class="blank1">
        <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
      </section>
      <section class="left">
        <h2>
          <font color="#fcdb03">M</font>
          <font color="#ffffff"> ROVER</font>
        </h2>

        <h3>Object Detector</h3>
        <p>
          As a part of <a href="https://urc.marssociety.org/">URC</a> the rover must be able to identify objects and localize them in 3D space. To accomplish this task, I spearheaded the development of a custom machine learning model. Inference is then accelerated using  <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> and <a href="https://www.nvidia.com/en-us/data-center/tensor-cores/">tensor cores</a> to achieve frequency of 15 Hz on the Jetson Xavier. With the objects identified, they are localized in 3D space using a stereo 3D pointcloud. 
        </p>
        <h3>Object Detector URC 2024</h3>
        <p>
		  <iframe width="560" height="315" src="https://www.youtube.com/embed/IvkKQNy5MkM?si=GzP2KuLfW_Z9Zq_Z" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
        <h3>Lander Align</h3>
        <p>
		  In the Equipment Servicing task the rover must be able to interact with a control panel. Due to the planar nature of the interface the approach can be automated. Using point cloud filtering based on normal vector the plane can be isolated and then modeled using RANSAC. Knowing the panel's location allows the rover to approach it using a <a href="https://hades.mech.northwestern.edu/images/7/7f/MR.pdf">nonlinear feedforward plus feedback control law</a>, putting the rover in the best possible position.
        </p>

      </section>
      <section class="blank2">
        <br><br><br><br><br><br><br><br><br><br><br><br>
      </section>

		
      <section class="right">
        <h2>
          <font color="#ffffff">Personal Projects</font>
        </h2>

        <p>
			High performance &#128200; and low level &#127899; applications peak my interest.
        </p>
        <h3>High Performance &#128640;</h3>
        <p>
			<ul style="list-style-type:circle">
			  <li><a href="https://github.com/jbrhm/CudaLibrary">Cupybara</a>: A high performance BLAS that utlizes <a href="https://developer.nvidia.com/cuda-toolkit">CUDA</a> </li>
			  <li>Coming Soon! Orientation Visualizer</li>
			</ul>
        </p>
        <h3>Low Level &#127898;</h3>
        <p>
			<ul style="list-style-type:circle">
			  <li><a href="https://github.com/jbrhm/AssemblyGame">Pong In Assembly</a>: Fully functional Pong Game written in assembly that interfaces directly to the X11 Server to create a GUI and render all of the game's assets.</li>
			</ul>

		<iframe width="560" height="315" src="https://www.youtube.com/embed/kc3R4MejQeI?si=sRDF1sNuE7M8_aCW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

		</p>
        <p>
			<ul style="list-style-type:circle">
			  <li><a href="https://github.com/jbrhm/Allocator">Memory Allocator</a>: An investigation into what <a href="https://www.gnu.org/software/libc/">libc</a> does under the hood when calling malloc and free.</li>
			</ul>
        </p>
      </section>

      <section class="blank2">
        <br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
      </section>

      <section class="left">
        <h2>
          <font color="#fcdb03">M</font>
          <font color="#ffffff"> Robotics</font>
        </h2>

        <h3>Spot</h3>
        <p>
          Working in the <a href="https://github.com/UM-ARM-Lab">ARM</a> lab I worked with <a href="https://bostondynamics.com/products/spot/">Spot</a> to improved its outdoor functionality. Here are some of the projects I worked on during the process.
        </p>
        <h3>Outdoor Localization</h3>
		<p>
			Using an Extended Kalman Filter, I fused the localization data from Spot's visual odometry and a GPS to create a more robust outdoor localization system.
		</p>
		<h3>Tool Retrieval</h3>
		<p>
			Worked with perception, navigation, and manipulation subsystems to integrate an ML based voice prompt tool retrieval pipeline.
		</p>
        <p>
		  <iframe width="560" height="315" src="https://www.youtube.com/embed/Yd32IJV93hE?si=1wJuGU06oJLGEezV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </p>
      </section>

	  <section class="blank2">
        <br><br><br><br><br>
      </section>

      <blockquote>
        <p>Thanks for reading!</p>
      </blockquote>


    </main>

    <script type="module" src="/src/main.js"></script>
  </body>
</html>
